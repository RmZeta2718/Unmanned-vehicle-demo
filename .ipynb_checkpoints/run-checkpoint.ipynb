{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "在这个 notebook 中，我们会展示如何使用 JetBot 跟踪对象。我们将使用在 [COCO dataset](http://cocodataset.org) 网站上预先训练好的神经网络，以检测90种不同的常见对象。例如包括了\n",
    "\n",
    "* 人 (index 0)\n",
    "* 杯子 (index 47)\n",
    " \n",
    "和许多其他的常见对象。你可以点击 [这里查看](https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_complete_label_map.pbtxt) 所有的对象。\n",
    "\n",
    "该模型来自 [TensorFlow 对象检测 API](https://github.com/tensorflow/models/tree/master/research/object_detection)，它还提供自定义训练任务，当训练完成，我们就能使用 Jetson Nano 上的 NVIDIA TensorRT 对其进行优化。\n",
    "\n",
    "这样做使得这个神经网络非常快，能在 Jetson 上实时执行。但是，这 notebook 并不包含所说的训练与优化步骤。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "\n",
    "detectWidth, detectHeight = 300, 300\n",
    "poseWidth, poseHeight = 224, 224\n",
    "\n",
    "colorNormal, colorTarget = (255, 0, 0), (0, 255, 0)\n",
    "thickNormal, thickTarget = 2, 5\n",
    "\n",
    "# layout = widgets.Layout(width='auto')#, height='40px')\n",
    "image_widget = widgets.Image(format='jpeg', width=detectWidth, height=detectHeight)\n",
    "target_label = widgets.Label(value='tracked target:')\n",
    "target_text = widgets.Text(value='Nothing')\n",
    "object_list = widgets.Label(value='')\n",
    "tolerance_widget = widgets.FloatSlider(value=0.2, min=0.0, max=0.5, description='Deviation tolerance')\n",
    "speed_widget = widgets.FloatSlider(value=0.4, min=0.0, max=1.0, description='speed')\n",
    "turn_gain_widget = widgets.FloatSlider(value=0.4, min=0.0, max=1.0, description='turn gain')\n",
    "error_widget = widgets.Label(value='')\n",
    "start_button = widgets.Button(description = \"Start\")\n",
    "capture_button = widgets.Button(description = \"Capture Now\")\n",
    "restart_button = widgets.Button(description = \"Stop tracking\")\n",
    "calibration_button = widgets.Button(description = \"Do calibration\")\n",
    "calib_load_button = widgets.Button(description = \"Load calibration\")\n",
    "calib_abandon_button = widgets.Button(description = \"Abandon calibration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面初始化相机。我需要300x300的图像像素作为输入。所以，我们需要设置我们的摄像头。\n",
    " \n",
    "> 在内部，Jetson Nano的图像信号处理是使用Carmera类的GStreamer实现的。这是超级快速的方式，大大地减少了CPU的计算量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Camera\n",
    "camera = Camera.instance(width=detectWidth, height=detectHeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from jetbot import bgr8_to_jpeg\n",
    "\n",
    "mtx_cal, dist_cal = None, None\n",
    "last_init_img = None\n",
    "\n",
    "def calibration_init(pic=None):\n",
    "    global mtx_cal, dist_cal, last_init_img\n",
    "    if pic is None:\n",
    "        pic = camera.value\n",
    "        last_init_img = np.copy(pic)\n",
    "#     patternSize = (19, 12)\n",
    "    patternSize = (4, 4)\n",
    "    patternWasFound, corners = cv2.findChessboardCorners(pic, patternSize)\n",
    "    # prepare object points, like (0,0,0), (1,0,0), (2,0,0), ...(6,5,0)\n",
    "    objp = np.zeros((patternSize[0] * patternSize[1], 3), np.float32)\n",
    "    objp[:, :2] = np.mgrid[0:patternSize[0], 0:patternSize[1]].T.reshape(-1, 2)\n",
    "\n",
    "    # Arrays to store object points and image points from all the images.\n",
    "    objpoints = [objp]  # 3d points in real world space\n",
    "    imgpoints = [corners]  # 2d pionts in image plane.\n",
    "    imgSize = (pic.shape[1], pic.shape[0])\n",
    "    rst, mtx_cal, dist_cal, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, imgSize, None, None)\n",
    "    \n",
    "def calibration_do(change):\n",
    "    change['new'] = cv2.undistort(change['new'], mtx_cal, dist_cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if last_init_img is not None:\n",
    "#     cv2.imwrite('best_calib_img.jpg', last_init_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果摄像头的视野中有任何COCO对象时，它们会存储在``detections``变量中。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化JetBot，这样我们就可以控制JetBot的电机了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the JSON file which describes the human pose task.  This is in COCO format, it is the category descriptor pulled from the annotations file.  We modify the COCO category slightly, to add a neck keypoint.  We will use this task description JSON to create a topology tensor, which is an intermediate data structure that describes the part linkages, as well as which channels in the part affinity field each linkage corresponds to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import trt_pose.coco\n",
    "# import trt_pose\n",
    "\n",
    "# print(trt_pose.__path__)\n",
    "with open('human_pose.json', 'r') as f:\n",
    "    human_pose = json.load(f)\n",
    "\n",
    "topology = trt_pose.coco.coco_category_to_topology(human_pose)\n",
    "# tag: left:[7, 9] right:[8, 10]\n",
    "# tag in topology [6, 8] [7, 9]\n",
    "# tag index 6. 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load our model.  Each model takes at least two parameters, *cmap_channels* and *paf_channels* corresponding to the number of heatmap channels\n",
    "and part affinity field channels.  The number of part affinity field channels is 2x the number of links, because each link has a channel corresponding to the\n",
    "x and y direction of the vector field for each link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trt_pose.models\n",
    "\n",
    "num_parts = len(human_pose['keypoints'])\n",
    "num_links = len(human_pose['skeleton'])\n",
    "\n",
    "model = trt_pose.models.resnet18_baseline_att(num_parts, 2 * num_links).cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could then load the saved model using *torch2trt* as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch2trt import TRTModule\n",
    "\n",
    "OPTIMIZED_MODEL = 'resnet18_baseline_att_224x224_A_epoch_249_trt.pth'\n",
    "model_trt = TRTModule()\n",
    "model_trt.load_state_dict(torch.load(OPTIMIZED_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrawObjects(object):\n",
    "    \n",
    "    def __init__(self, topology):\n",
    "        self.topology = topology\n",
    "        \n",
    "    def __call__(self, image, object_counts, objects, normalized_peaks):\n",
    "        topology = self.topology\n",
    "        height = image.shape[0]\n",
    "        width = image.shape[1]\n",
    "        \n",
    "        count = int(object_counts[0])\n",
    "        K = topology.shape[0]\n",
    "        \n",
    "        rst = []\n",
    "        \n",
    "        for i in range(count):\n",
    "            obj = objects[0][i]\n",
    "            C = obj.shape[0]\n",
    "            for j in range(C):\n",
    "                k = int(obj[j])\n",
    "                if k >= 0:\n",
    "                    peak = normalized_peaks[0][j][k]\n",
    "                    x = round(float(peak[1]) * width)\n",
    "                    y = round(float(peak[0]) * height)\n",
    "                    if j == 8 or j == 10:\n",
    "                        cv2.circle(image, (x, y), 3, colorTarget, thickTarget)\n",
    "                    else:\n",
    "                        cv2.circle(image, (x, y), 3, colorNormal, thickNormal)\n",
    "\n",
    "            for k in range(K):\n",
    "                c_a = topology[k][2]\n",
    "                c_b = topology[k][3]\n",
    "                if obj[c_a] >= 0 and obj[c_b] >= 0:\n",
    "                    peak0 = normalized_peaks[0][c_a][obj[c_a]]\n",
    "                    peak1 = normalized_peaks[0][c_b][obj[c_b]]\n",
    "                    x0 = round(float(peak0[1]) * width)\n",
    "                    y0 = round(float(peak0[0]) * height)\n",
    "                    x1 = round(float(peak1[1]) * width)\n",
    "                    y1 = round(float(peak1[0]) * height)\n",
    "                    if k == 8:\n",
    "                        cv2.line(image, (x0, y0), (x1, y1), colorTarget, thickTarget)\n",
    "                        rst.append((peak0[1], peak0[0], peak1[1], peak1[0]))\n",
    "                    else:\n",
    "                        cv2.line(image, (x0, y0), (x1, y1), colorNormal, thickNormal)\n",
    "        return rst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define two callable classes that will be used to parse the objects from the neural network, as well as draw the parsed objects on an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from trt_pose.draw_objects import DrawObjects    # 因为要修改，所以复制到上面了\n",
    "from trt_pose.parse_objects import ParseObjects\n",
    "\n",
    "parse_objects = ParseObjects(topology)\n",
    "draw_objects = DrawObjects(topology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll define the main execution loop.  This will perform the following steps\n",
    "\n",
    "1.  Preprocess the camera image\n",
    "2.  Execute the neural network\n",
    "3.  Parse the objects from the neural network output\n",
    "4.  Draw the objects onto the camera image\n",
    "5.  Convert the image to JPEG format and stream to the display widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiPersonWarning():\n",
    "    global modified\n",
    "    modified = True\n",
    "    error_widget.value += 'Warning: More than one person found in the image.\\n'\n",
    "\n",
    "# counts, objects, peaks = None\n",
    "def executePose(change):\n",
    "#     global counts, objects, peaks\n",
    "    image = change['new']\n",
    "    if 'output' not in change:\n",
    "        change['output'] = np.copy(change['new'])\n",
    "    output = change['output']\n",
    "    \n",
    "    data = preprocess(image)\n",
    "    cmap, paf = model_trt(data)\n",
    "    cmap, paf = cmap.detach().cpu(), paf.detach().cpu()\n",
    "    counts, objects, peaks = parse_objects(cmap, paf)#, cmap_threshold=0.15, link_threshold=0.15)\n",
    "    arms = draw_objects(output, counts, objects, peaks)\n",
    "    if len(arms) > 1:\n",
    "        multiPersonWarning()\n",
    "    change['lines'] = arms\n",
    "#     print(objects)\n",
    "#     image_widget.value = bgr8_to_jpeg(image[:, ::-1, :])\n",
    "    \n",
    "#     print(\"(%.2f, %.2f), (%.2f, %.2f)\" % (x0, y0, x1, y1))\n",
    "# executePose({'new': camera.value})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算单个摄像头图像的信号"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们要导入``ObjectDetector``类，它采用我们预先训练过的SSD引擎。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['background', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'unknown', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'unknown', 'backpack', 'umbrella', 'unknown', 'unknown', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'unknown', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'unknown', 'dining table', 'unknown', 'unknown', 'toilet', 'unknown', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'unknown', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n"
     ]
    }
   ],
   "source": [
    "from jetbot import ObjectDetector\n",
    "\n",
    "detectModel = ObjectDetector('ssd_mobilenet_v2_coco.engine')\n",
    "\n",
    "with open('detection_target.json', 'r') as f:\n",
    "    detection_targets = [s if not s.isnumeric() else 'unknown' for s in json.load(f)['name']]\n",
    "print(detection_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在内部，``ObjectDetector``类使用 TensorRT Python API执行我们提供的引擎。\n",
    "  \n",
    "它还负责预处理神经网络的输入，以及分析检测到的对象。\n",
    "\n",
    "现在这只适用于``jetbot.ssd_tensorrt``package创建的引擎。该软件包具有转换阻最佳的TensorFlow对象模型给TensorRT引擎\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，让我们使用一些摄像头输入执行我们的神经网络。默认情况下``ObjectDetector`` 类生成 ``bgr8``格式。然而，如果你想使用不同的格式作为输入，则你可以覆盖默认的预处理功能。 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detections = detectModel(camera.value)\n",
    "# print(detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 显示检测对象的名称\n",
    "\n",
    "我们将使用下面的代码打印出检测到的对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# detections_widget = widgets.Textarea()\n",
    "\n",
    "# detections_widget.value = str(detections)\n",
    "\n",
    "# display(detections_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你应该看到每个图像中检测到的每个标签，相似度和边界框。但在这个例子只有一个图像。\n",
    "\n",
    "\n",
    "要打印第一个图像中检测到的第一个对象，我们可以调用一下内容\n",
    "\n",
    "> 如果未检测到任何对象，则可能会抛出错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from jetbot import bgr8_to_jpeg\n",
    "# import traitlets\n",
    "# import ipywidgets.widgets as widgets\n",
    "# image = widgets.Image(format='jpeg', width=300, height=300)\n",
    "\n",
    "# display(image)\n",
    "# camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 控制Jetbot去跟踪中心目标\n",
    "\n",
    "现在我们希望我们的JetBot能跟随指定的对象。 为此，我们将执行以下操作\n",
    "\n",
    "1. 检测与指定匹配的对象\n",
    "\n",
    "2. 选择最接近摄像机视野中心的物体，就是 \"目标\" 对象\n",
    "\n",
    "3. 将机器人转向目标物体，否则徘徊\n",
    "\n",
    "4. 如果我们被障碍物阻挡，则左转  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# collision_model = torchvision.models.alexnet(pretrained=False)\n",
    "# collision_model.classifier[6] = torch.nn.Linear(collision_model.classifier[6].in_features, 2)\n",
    "# collision_model.load_state_dict(torch.load('best_model.pth'))\n",
    "device = torch.device('cuda')\n",
    "# collision_model = collision_model.to(device)\n",
    "\n",
    "mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "\n",
    "def preprocess(camera_value):\n",
    "    global device, normalize\n",
    "    x = camera_value\n",
    "    x = cv2.resize(x, (poseWidth, poseHeight))\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)\n",
    "    x = x[None, ...]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，让我们显示所有的控件在网络上执行代码更新摄像头。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bbox = (left, bottom, right, top) \\in [0, 1]\n",
    "\n",
    "def drawRect(image, bbox, color, thickness):\n",
    "    cv2.rectangle(\n",
    "        image,\n",
    "        (int(detectWidth * bbox[0]), int(detectHeight * bbox[1])), \n",
    "        (int(detectWidth * bbox[2]), int(detectHeight * bbox[3])),\n",
    "        color,\n",
    "        thickness\n",
    "    )\n",
    "\n",
    "def multiTargetWarning():\n",
    "    global modified\n",
    "    modified = True\n",
    "    error_widget.value += 'Warning: More than one target found in the image.\\n'\n",
    "    \n",
    "def executeDetect(change):\n",
    "    image = change['new']\n",
    "    if 'output' not in change:\n",
    "        change['output'] = np.copy(change['new'])\n",
    "    output = change['output']\n",
    "    \n",
    "    # 计算所有被检测到的目标，不包含人\n",
    "    detections = [det for det in detectModel(image)[0] if det['label'] != 1]\n",
    "    # why [0] ? might be batch index, where batch size is 1\n",
    "    \n",
    "    # 将所有目标绘制在图像上\n",
    "    for det in detections:\n",
    "        drawRect(output, det['bbox'], colorNormal, thickNormal)\n",
    "        \n",
    "        \n",
    "    if 'target' in change:    # True only in status 2\n",
    "        matching_detections = [d for d in detections if d['label'] == change['target']]\n",
    "        for match in matching_detections:\n",
    "            drawRect(output, match['bbox'], colorTarget, thickTarget)\n",
    "        if len(matching_detections) > 1:\n",
    "            multiTargetWarning()\n",
    "    change['detections'] = detections\n",
    "#     robot.forward(float(speed_widget.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def detection_center(detection):\n",
    "    \"\"\"计算目标的xy中心坐标\"\"\"\n",
    "    bbox = detection['bbox']\n",
    "#     print(\"bbox:\", bbox)\n",
    "    center_x = (bbox[0] + bbox[2]) / 2.0\n",
    "    center_y = (bbox[1] + bbox[3]) / 2.0\n",
    "#     print(\"cx, cy:\", center_x, center_y)\n",
    "    return center_x, center_y\n",
    "\n",
    "def cosineSimilarity(a, b):\n",
    "    return dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "def executeFind(change):\n",
    "    if len(change['lines']) == 0 or len(change['detections']) == 0:\n",
    "        return\n",
    "    \n",
    "    image = change['new']\n",
    "    if 'output' not in change:\n",
    "        change['output'] = np.copy(change['new'])\n",
    "    output = change['output']\n",
    "    detections = change['detections']\n",
    "    \n",
    "#     x0, y0, x1, y1 = 200, 0, 200, 10\n",
    "    x0, y0, x1, y1 = change['lines'][0]\n",
    "    p0 = np.array([x0, y0])\n",
    "    p1 = np.array([x1, y1])\n",
    "    \n",
    "#     print('--------')\n",
    "#     print('p0', p0)\n",
    "#     print('p1', p1)\n",
    "    \n",
    "    bestApproch = None\n",
    "    bestSimilarity = float('-inf')\n",
    "    for det in detections:\n",
    "        p = np.array(detection_center(det))\n",
    "#         print('centerxy:', p)\n",
    "        similarity =  cosineSimilarity(p1 - p0, p - p0)\n",
    "        if similarity > bestSimilarity:\n",
    "            bestApproch = det\n",
    "            bestSimilarity = similarity\n",
    "#     print(bestSimilarity)\n",
    "    if bestSimilarity > 0.8:\n",
    "        change['target'] = bestApproch['label']\n",
    "        drawRect(output, bestApproch['bbox'], colorTarget, thickTarget)\n",
    "#     if bestApproch is None:\n",
    "#         print(detections)\n",
    "#     else:\n",
    "#         cv2.imwrite('image.jpg', output)\n",
    "#         shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_size(detection):\n",
    "    # 计算边界框的面积\n",
    "    bbox = detection['bbox']\n",
    "    return (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n",
    "\n",
    "def robotForward(speed):\n",
    "    if 0 < speed <= 0.2:\n",
    "        robot.forward(0.2)\n",
    "        time.sleep(0.01)\n",
    "    if -0.2 <= speed < 0:\n",
    "        robot.forward(-0.2)\n",
    "        time.sleep(0.01)\n",
    "    robot.forward(speed)\n",
    "    \n",
    "def robotRight(speed):\n",
    "    if 0 < speed <= 0.2:\n",
    "        robot.right(0.2)\n",
    "        time.sleep(0.01)\n",
    "    if -0.2 <= speed < 0:\n",
    "        robot.right(-0.2)\n",
    "        time.sleep(0.01)\n",
    "    robot.right(speed)\n",
    "    \n",
    "def executeFollow(change):\n",
    "    # 执行碰撞模型以确定是否被阻止\n",
    "#     collision_output = collision_model(preprocess(change['new'])).detach().cpu()\n",
    "#     prob_blocked = float(F.softmax(collision_output.flatten(), dim=0)[0])\n",
    "#     change['blocked'] = prob_blocked\n",
    "#     if prob_blocked > 0.5:\n",
    "#         pass\n",
    "    \n",
    "    targets = [det for det in change['detections'] if det['label'] == change['target']]\n",
    "    if len(targets) == 0:\n",
    "#         print('no target found')\n",
    "        robot.stop()\n",
    "        return\n",
    "#         shutdown()\n",
    "    center_x, center_y = detection_center(targets[0])    # ignore all other targets except the first one\n",
    "    center_x, center_y = center_x - 0.5, center_y - 0.5  # move origin to the center of image\n",
    "    size = detection_size(targets[0])\n",
    "#     robot.set_motors(\n",
    "#         0.5 * float(speed_widget.value + turn_gain_widget.value * center_x),\n",
    "#         0.5 * float(speed_widget.value - turn_gain_widget.value * center_x)\n",
    "#     )\n",
    "    if abs(center_x) > tolerance_widget.value:\n",
    "        robotRight(turn_gain_widget.value * center_x / 0.5)\n",
    "    else:\n",
    "        robotForward(speed_widget.value * (0.5 - size) / 0.5)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def targetNotFoundError():\n",
    "    global modified\n",
    "    modified = True\n",
    "    error_widget.value += 'ERROR: No target object found, please click the button when appropriate.\\n'\n",
    "\n",
    "\n",
    "def clearAllWarnings():\n",
    "    error_widget.value = ''\n",
    "\n",
    "status = -1\n",
    "target = None    # target label\n",
    "modified = False\n",
    "doCalib = False\n",
    "\n",
    "def execute(change):\n",
    "    try:\n",
    "        modified = False\n",
    "        global status, target\n",
    "    #     print('output' in change)    # always False\n",
    "        if doCalib:\n",
    "            calibration_do(change)\n",
    "        if status == -1:\n",
    "            image_widget.value = bgr8_to_jpeg(change['new'])\n",
    "            return\n",
    "        if status == 0 or status == 1:\n",
    "            executePose(change)\n",
    "            executeDetect(change)\n",
    "            executeFind(change)\n",
    "            if 'target' in change:\n",
    "                target_text.value = detection_targets[change['target']]\n",
    "            else:\n",
    "                target_text.value = 'Nothing'\n",
    "            if status == 1:\n",
    "                if 'target' in change:\n",
    "                    target = change['target']\n",
    "                    status = 2\n",
    "                else:\n",
    "                    targetNotFoundError()\n",
    "                    status = 0\n",
    "        elif status == 2:\n",
    "            change['target'] = target\n",
    "            executeDetect(change)\n",
    "            executeFollow(change)\n",
    "        if not modified:\n",
    "            clearAllWarnings()\n",
    "        image_widget.value = bgr8_to_jpeg(change['output'])\n",
    "        object_list.value = ' '.join([detection_targets[det['label']] for det in change['detections']])\n",
    "    \n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "        shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`camera.observe()` 在另一个线程里监视摄像头并调用 `execute()` ，所以摄像头停止监视后，要等待一小段时间，等待 `execute()` 退出，再最终令电机停止。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def shutdown():\n",
    "    camera.unobserve_all()\n",
    "    time.sleep(0.05)\n",
    "    robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calib_click(sender):\n",
    "    global doCalib\n",
    "    calibration_init()\n",
    "    doCalib = True\n",
    "    \n",
    "def calib_load_click(sender):\n",
    "    global doCalib\n",
    "    calibration_init(cv2.imread('best_calib_img.jpg'))\n",
    "    doCalib = True\n",
    "    \n",
    "def abdcalib_click(sender):\n",
    "    global doCalib\n",
    "    doCalib = False\n",
    "    mtx_cal, dist_cal = None, None\n",
    "    \n",
    "def start_click(sender):\n",
    "    global status\n",
    "    status = 0\n",
    "    \n",
    "def cap_click(sender):\n",
    "    global status\n",
    "    status = 1\n",
    "    \n",
    "def restart_click(sender):\n",
    "    global status\n",
    "    status = 0\n",
    "    robot.stop()\n",
    "    \n",
    "\n",
    "calibration_button.on_click(calib_click)\n",
    "calib_load_button.on_click(calib_load_click)\n",
    "calib_abandon_button.on_click(abdcalib_click)\n",
    "start_button.on_click(start_click)\n",
    "capture_button.on_click(cap_click)\n",
    "restart_button.on_click(restart_click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8390fb658644e428802ca5a53f7ee09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Image(value=b'', format='jpeg', height='300', width='300'), VBox(children=(HBox(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(widgets.VBox([\n",
    "    widgets.HBox([\n",
    "        image_widget, \n",
    "        widgets.VBox([\n",
    "            widgets.HBox([calib_load_button, calibration_button, calib_abandon_button]),\n",
    "            speed_widget,\n",
    "            turn_gain_widget,\n",
    "            tolerance_widget,\n",
    "            widgets.HBox([start_button, capture_button, restart_button]),\n",
    "            error_widget,\n",
    "        ]),\n",
    "    ]),\n",
    "    widgets.HBox([target_label, target_text]),\n",
    "    object_list,\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用下面的代码，将执行功能连接到每帧图像更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "camera.unobserve_all()\n",
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# robot.forward(0.2)\n",
    "# time.sleep(0.01)\n",
    "# robot.forward(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(0.2)\n",
    "robot.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
